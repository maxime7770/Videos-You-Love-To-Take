{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " frame_input (InputLayer)    [(None, 45, 1024)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 45, 10)               51210     ['frame_input[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 45, 10)               40        ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 45, 10)               0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 45, 10)               510       ['leaky_re_lu_6[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 45, 10)               40        ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 45, 10)               0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 45, 10)               510       ['leaky_re_lu_7[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 45, 10)               40        ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)               (None, 45, 512)              3147776   ['frame_input[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 45, 10)               0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)               (None, 45, 256)              787456    ['lstm_6[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 10)                   0         ['leaky_re_lu_8[0][0]']       \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)               (None, 128)                  197120    ['lstm_7[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 138)                  0         ['global_average_pooling1d_2[0\n",
      " )                                                                  ][0]',                        \n",
      "                                                                     'lstm_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 128)                  17792     ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 128)                  0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 64)                   8256      ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 64)                   0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 10)                   650       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4211400 (16.07 MB)\n",
      "Trainable params: 4211340 (16.06 MB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 17:21:07.975669: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 15s 91ms/step - loss: 0.4006 - accuracy: 0.2100 - val_loss: 0.3817 - val_accuracy: 0.2873\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 11s 78ms/step - loss: 0.2842 - accuracy: 0.4612 - val_loss: 0.4826 - val_accuracy: 0.1214\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 10s 76ms/step - loss: 0.2149 - accuracy: 0.5933 - val_loss: 0.3516 - val_accuracy: 0.3545\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 11s 77ms/step - loss: 0.1920 - accuracy: 0.6392 - val_loss: 0.3039 - val_accuracy: 0.5333\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 11s 77ms/step - loss: 0.1757 - accuracy: 0.6747 - val_loss: 0.2128 - val_accuracy: 0.5252\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 11s 76ms/step - loss: 0.1613 - accuracy: 0.6997 - val_loss: 0.1435 - val_accuracy: 0.7084\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 10s 75ms/step - loss: 0.1499 - accuracy: 0.7199 - val_loss: 0.1156 - val_accuracy: 0.7567\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 11s 76ms/step - loss: 0.1422 - accuracy: 0.7427 - val_loss: 0.2460 - val_accuracy: 0.5156\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 11s 80ms/step - loss: 0.1375 - accuracy: 0.7508 - val_loss: 0.2143 - val_accuracy: 0.6203\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 11s 77ms/step - loss: 0.1324 - accuracy: 0.7781 - val_loss: 0.1146 - val_accuracy: 0.8018\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 11s 78ms/step - loss: 0.1261 - accuracy: 0.7950 - val_loss: 0.1354 - val_accuracy: 0.7723\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 10s 75ms/step - loss: 0.1186 - accuracy: 0.8104 - val_loss: 0.1217 - val_accuracy: 0.7734\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 11s 75ms/step - loss: 0.1163 - accuracy: 0.8217 - val_loss: 0.1339 - val_accuracy: 0.7411\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 10s 75ms/step - loss: 0.1165 - accuracy: 0.8232 - val_loss: 0.2893 - val_accuracy: 0.5215\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 10s 76ms/step - loss: 0.1102 - accuracy: 0.8367 - val_loss: 0.2253 - val_accuracy: 0.7170\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 12s 93ms/step - loss: 0.1086 - accuracy: 0.8347 - val_loss: 0.6146 - val_accuracy: 0.2374\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 11s 81ms/step - loss: 0.1050 - accuracy: 0.8457 - val_loss: 0.6428 - val_accuracy: 0.4216\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 12s 88ms/step - loss: 0.1026 - accuracy: 0.8491 - val_loss: 0.1522 - val_accuracy: 0.7718\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 11s 78ms/step - loss: 0.0965 - accuracy: 0.8546 - val_loss: 0.0994 - val_accuracy: 0.8394\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 11s 79ms/step - loss: 0.0946 - accuracy: 0.8600 - val_loss: 0.0871 - val_accuracy: 0.8631\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 11s 76ms/step - loss: 0.0907 - accuracy: 0.8668 - val_loss: 0.0707 - val_accuracy: 0.8888\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 11s 79ms/step - loss: 0.0902 - accuracy: 0.8673 - val_loss: 0.0799 - val_accuracy: 0.8496\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 11s 76ms/step - loss: 0.0901 - accuracy: 0.8661 - val_loss: 0.0628 - val_accuracy: 0.8969\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 10s 76ms/step - loss: 0.0904 - accuracy: 0.8729 - val_loss: 0.0801 - val_accuracy: 0.8749\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 11s 79ms/step - loss: 0.0841 - accuracy: 0.8786 - val_loss: 0.2194 - val_accuracy: 0.7025\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 11s 77ms/step - loss: 0.0821 - accuracy: 0.8814 - val_loss: 0.0630 - val_accuracy: 0.8926\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 11s 79ms/step - loss: 0.0842 - accuracy: 0.8805 - val_loss: 0.0671 - val_accuracy: 0.8990\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 11s 77ms/step - loss: 0.0823 - accuracy: 0.8792 - val_loss: 0.0598 - val_accuracy: 0.9039\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 11s 76ms/step - loss: 0.0809 - accuracy: 0.8831 - val_loss: 1.2138 - val_accuracy: 0.3190\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 11s 81ms/step - loss: 0.0827 - accuracy: 0.8794 - val_loss: 0.0799 - val_accuracy: 0.8845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28aaf3520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Conv1D, BatchNormalization, LeakyReLU, concatenate, GlobalAveragePooling1D, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "\n",
    "# class TransformerBlock(layers.Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "#         self.ffn = tf.keras.Sequential(\n",
    "#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "#         )\n",
    "#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout1 = layers.Dropout(rate)\n",
    "#         self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "#     def call(self, inputs, training):\n",
    "#         attn_output = self.att(inputs, inputs)\n",
    "#         attn_output = self.dropout1(attn_output, training=training)\n",
    "#         out1 = self.layernorm1(inputs + attn_output)\n",
    "#         ffn_output = self.ffn(out1)\n",
    "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
    "#         return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "\n",
    "# CNN Block\n",
    "def cnn_block(x, filters, kernel_size, strides, activation):\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation == 'leaky_relu':\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "    elif activation == 'relu':\n",
    "        x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# Parameters\n",
    "num_frames = 45  # Adjust to match the fixed number of frames you've processed per video\n",
    "num_features = 1024  # Feature dimension per frame\n",
    "batch_size = 64\n",
    "number_of_labels = 10\n",
    "\n",
    "# Load the dataset\n",
    "load_dir = './saved_dataset'\n",
    "loaded_dataset = tf.data.experimental.load(load_dir)\n",
    "\n",
    "# Shuffle and prepare the datasets\n",
    "shuffled_dataset = loaded_dataset.shuffle(buffer_size=10000)\n",
    "split_size = int(0.8 * len(shuffled_dataset))\n",
    "train_dataset = shuffled_dataset.take(split_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = shuffled_dataset.skip(split_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Input for the frame sequence\n",
    "frame_input = Input(shape=(num_frames, num_features), name='frame_input')\n",
    "\n",
    "# CNN Branch\n",
    "cnn_branch = cnn_block(frame_input, filters=10, kernel_size=5, strides=1, activation='leaky_relu')\n",
    "cnn_branch = cnn_block(cnn_branch, filters=10, kernel_size=5, strides=1, activation='leaky_relu')\n",
    "cnn_branch = cnn_block(cnn_branch, filters=10, kernel_size=5, strides=1, activation='leaky_relu')\n",
    "cnn_branch_output = GlobalAveragePooling1D()(cnn_branch)  # Aggregate the features from the CNN branch\n",
    "\n",
    "# LSTM Branch\n",
    "lstm_branch = LSTM(512, return_sequences=True)(frame_input)\n",
    "lstm_branch = LSTM(256, return_sequences=True)(lstm_branch)\n",
    "lstm_branch_output = LSTM(128, return_sequences=False)(lstm_branch)\n",
    "\n",
    "# Transformer Branch\n",
    "# transformer_branch = layers.Dense(512, activation=\"relu\")(frame_input)  \n",
    "# transformer_branch = TransformerBlock(512, 8, 64)(transformer_branch)\n",
    "# transformer_branch = layers.GlobalAveragePooling1D()(transformer_branch)\n",
    "# transformer_branch = layers.Dropout(0.5)(transformer_branch)\n",
    "# transformer_branch_output = layers.Dense(128, activation=\"relu\")(transformer_branch)\n",
    "\n",
    "# Combine CNN and LSTM branch outputs and transformer branch output\n",
    "combined_output = concatenate([cnn_branch_output, lstm_branch_output])\n",
    "\n",
    "# Final Dense layers\n",
    "x = Dense(128, activation='relu')(combined_output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(number_of_labels, activation='sigmoid')(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=frame_input, outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model with validation\n",
    "model.fit(train_dataset, epochs=30, validation_data=validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to .h5\n",
    "\n",
    "model.save('model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
